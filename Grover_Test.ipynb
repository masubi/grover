{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grover-Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masubi/grover/blob/master/Grover_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1ANYnHVvfeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install bert-pytorch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skx6hMPixnF5",
        "colab_type": "code",
        "outputId": "905896d5-bcdc-4ac9-95f6-b8c5d4016558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id43yVeR0l4m",
        "colab_type": "code",
        "outputId": "20ea6fca-1d71-49ff-c5ae-4bd13f510ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/gdrive/My Drive/NLP_Projects/grover-master"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/NLP_Projects/grover-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcyvLmkL0vLq",
        "colab_type": "code",
        "outputId": "34335fd5-4aad-486b-cbf8-4eb29ce438cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/NLP_Projects/grover-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8Y90vzZ6zeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python3 download_model.py base #run if it hasn't downloaded model yet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87ZYWw0o1P2n",
        "colab_type": "code",
        "outputId": "dddbdba2-c06a-4d7a-cb21-646a0a9d4f6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "\"\"\"\n",
        "For discrimination finetuning (e.g. saying whether or not the generation is human/grover)\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.lib.io import file_io\n",
        "\n",
        "from lm.dataloader import classification_convert_examples_to_features, classification_input_fn_builder\n",
        "from lm.modeling import classification_model_fn_builder, GroverConfig\n",
        "from lm.utils import _save_np\n",
        "from sample.encoder import get_encoder"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/optimization_adafactor.py:88: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5gIvfE61dS4",
        "colab_type": "code",
        "outputId": "70298d26-9b5a-4c9f-b925-00c56d1d68d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "flags = tf.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "#\n",
        "# Clearing flags in this section so can rerun,  FLAGS are global which is why\n",
        "# error kept occurring \n",
        "#\n",
        "configListToRemove = [\"config_file\", \n",
        "                      \"input_data\", \n",
        "                      \"additional_data\", \n",
        "                      \"output_dir\",\n",
        "                      \"init_checkpoint\",\n",
        "                      \"max_seq_length\",\n",
        "                      \"iterations_per_loop\",\n",
        "                      \"batch_size\",\n",
        "                      \"max_training_examples\",\n",
        "                      \"do_train\",\n",
        "                      \"predict_val\",\n",
        "                      \"predict_test\",\n",
        "                      \"num_train_epochs\",\n",
        "                      \"warmup_proportion\",\n",
        "                      \"adafactor\",\n",
        "                      \"learning_rate\",\n",
        "                      \"use_tpu\",\n",
        "                      \"tpu_name\",\n",
        "                      \"tpu_zone\",\n",
        "                      \"gcp_project\",\n",
        "                      \"master\",\n",
        "                      \"num_tpu_cores\",\n",
        "                      \"f\"\n",
        "                      ]\n",
        "                \n",
        "for name in list(flags.FLAGS):\n",
        "  if(name in configListToRemove):\n",
        "    delattr(flags.FLAGS,name)\n",
        "    print(\"pre-delAttr: \" + name)\n",
        "\n",
        "#for name in list(flags.FLAGS):\n",
        "#  \n",
        "## Required parameters\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"config_file\", 'lm/configs/base.json',\n",
        "    \"The config json file corresponding to the pre-trained news model. \"\n",
        "    \"This specifies the model architecture.\")\n",
        "      \n",
        "flags.DEFINE_string(\n",
        "    \"input_data\", '/content/gdrive/My Drive/NLP_Projects/grover-master/discrimination/input/input_small.jsonl',\n",
        "    \"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"additional_data\", None,\n",
        "    \"Should we provide additional input data? maybe.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"output_dir\", '/content/gdrive/My Drive/NLP_Projects/grover-master/discrimination/output/',\n",
        "    \"The output directory where the model checkpoints will be written.\")\n",
        "\n",
        "## Other parameters\n",
        "flags.DEFINE_string(\n",
        "    \"init_checkpoint\", None,\n",
        "    \"Initial checkpoint (usually from a pre-trained model).\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_seq_length\", 1024,\n",
        "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
        "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
        "    \"than this will be padded. Must match data generation.\")\n",
        "\n",
        "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
        "                     \"How many steps to make in each estimator call.\")\n",
        "\n",
        "flags.DEFINE_integer(\"batch_size\", 32, \"Batch size used\")\n",
        "\n",
        "flags.DEFINE_integer(\"max_training_examples\", -1, \"if you wanna limit the number\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n",
        "\n",
        "flags.DEFINE_bool(\"predict_val\", False, \"Whether to run eval on the dev set.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"predict_test\", False,\n",
        "    \"Whether to run the model in inference mode on the test set.\")\n",
        "\n",
        "flags.DEFINE_float(\"num_train_epochs\", 3.0,\n",
        "                   \"Total number of training epochs to perform.\")\n",
        "\n",
        "flags.DEFINE_float(\n",
        "    \"warmup_proportion\", 0.1,\n",
        "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
        "    \"E.g., 0.1 = 10% of training.\")\n",
        "\n",
        "flags.DEFINE_bool(\"adafactor\", False, \"Whether to run adafactor\")\n",
        "\n",
        "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
        "\n",
        "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"tpu_name\", None,\n",
        "    \"The Cloud TPU to use for training. This should be either the name \"\n",
        "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
        "    \"url.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"tpu_zone\", None,\n",
        "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"gcp_project\", None,\n",
        "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"num_tpu_cores\", 8,\n",
        "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "print(\"flags loaded\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "flags loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hepD3i8u-jx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _flatten_and_tokenize_metadata(encoder, item):\n",
        "    \"\"\"\n",
        "    Turn the article into tokens\n",
        "    :param item: Contains things that need to be tokenized\n",
        "\n",
        "    fields are ['domain', 'date', 'authors', 'title', 'article', 'summary']\n",
        "    :return: dict\n",
        "    \"\"\"\n",
        "    metadata = []\n",
        "    for key in ['domain', 'date', 'authors', 'title', 'article']:\n",
        "        val = item.get(key, None)\n",
        "        if val is not None:\n",
        "            metadata.append(encoder.__dict__[f'begin_{key}'])\n",
        "            metadata.extend(encoder.encode(val))\n",
        "            metadata.append(encoder.__dict__[f'end_{key}'])\n",
        "    return metadata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50NqMhI1-xBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "LABEL_LIST = ['machine', 'human']\n",
        "LABEL_INV_MAP = {label: i for i, label in enumerate(LABEL_LIST)}\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2u9kQooH280",
        "colab_type": "code",
        "outputId": "74ee8da9-2e72-4f1d-e3ad-35eb14f9ab40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "def checkFileConfigs():\n",
        "  if tf.gfile.Exists(FLAGS.output_dir):\n",
        "      print(f\"The output directory {FLAGS.output_dir} exists!\")\n",
        "      #if FLAGS.do_train:\n",
        "      #    print(\"EXITING BECAUSE DO_TRAIN is true\", flush=True)\n",
        "      #       return\n",
        "      for split in ['val', 'test']:\n",
        "          if tf.gfile.Exists(os.path.join(FLAGS.output_dir, f'{split}-probs.npy')) and getattr(FLAGS,\n",
        "                                                                                                f'predict_{split}'):\n",
        "              print(f\"EXITING BECAUSE {split}-probs.npy exists\", flush=True)\n",
        "              return\n",
        "  elif not FLAGS.do_train:\n",
        "      print(\"EXITING BECAUSE DO_TRAIN IS FALSE AND PATH DOESNT EXIST\")\n",
        "      return\n",
        "  #else:\n",
        "      #tf.gfile.MakeDirs(FLAGS.output_dir)\n",
        "  print(\"complete checking\")\n",
        "\n",
        "checkFileConfigs()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The output directory /content/gdrive/My Drive/NLP_Projects/grover-master/discrimination/output/ exists!\n",
            "complete checking\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCo4LyO_-1iU",
        "colab_type": "code",
        "outputId": "f5b16db0-693e-4a07-8292-648d548a974b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "news_config = GroverConfig.from_json_file(FLAGS.config_file)\n",
        "\n",
        "# TODO might have to change this\n",
        "encoder = get_encoder()\n",
        "examples = {'train': [], 'val': [], 'test': []}\n",
        "np.random.seed(123456)\n",
        "tf.logging.info(\"*** Parsing files ***\")\n",
        "with tf.gfile.Open(FLAGS.input_data, \"r\") as f:\n",
        "  print(\"f: \"+str(f.name))\n",
        "  for l in f:\n",
        "    print(\"l: \"+l)\n",
        "    item = json.loads(l)\n",
        "\n",
        "    # This little hack is because we don't want to tokenize the article twice\n",
        "    context_ids = _flatten_and_tokenize_metadata(encoder=encoder, item=item)\n",
        "    examples[item['split']].append({\n",
        "        'info': item,\n",
        "        'ids': context_ids,\n",
        "        'label': item['label'],\n",
        "    })\n",
        "    \n",
        "    assert item['label'] in LABEL_INV_MAP\n",
        "\n",
        "additional_data = {'machine': [], 'human': []}\n",
        "if FLAGS.additional_data is not None:\n",
        "    print(\"NOW WERE LOOKING AT ADDITIONAL INPUT DATA\", flush=True)\n",
        "    with tf.gfile.Open(FLAGS.additional_data, \"r\") as f:\n",
        "        for l in f:\n",
        "            item = json.loads(l)\n",
        "            # This little hack is because we don't want to tokenize the article twice\n",
        "            context_ids = _flatten_and_tokenize_metadata(encoder=encoder, item=item)\n",
        "            additional_data[item['label']].append({\n",
        "                'info': item,\n",
        "                'ids': context_ids,\n",
        "                'label': item['label'],\n",
        "            })\n",
        "\n",
        "print(\"examples[train]: \" + str(len(examples['train'])))\n",
        "print(\"examples[val]: \" + str(len(examples['val'])))\n",
        "print(\"examples[test]: \" + str(len(examples['test'])))\n",
        "tf.logging.info(\"*** Done parsing files ***\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/modeling.py:87: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "INFO:tensorflow:*** Parsing files ***\n",
            "f: /content/gdrive/My Drive/NLP_Projects/grover-master/discrimination/input/input_small.jsonl\n",
            "l: {\"article\": \"Patrons crowd the platform at the Washington Metropolitan Area Transit Authority's (WMATA) Metro Center stop in Washington, D.C. on Dec. 20, 2004. Thousands use the public transit system daily to get them in and around the D.C. area. (Karen Bleier/AFP/Getty Images)\\nUS Senators Threaten Metro Funding Over Chinese Manufacturer\\nWASHINGTON \\u2014Federal lawmakers say they\\u2019ll approve badly needed funding for Washington\\u2019s transit system, but only if it avoids buying new rail cars from China.\\nThe Washington Post reported on April 13 that U.S. Senators from Virginia and Maryland proposed the idea in new legislation. It reflects growing concerns that China\\u2019s state-owned rail company could hurt American manufacturers and make the system vulnerable to cyber espionage.\\nDave Smolensky, spokesman for the China Railway Rolling Stock Corp, dismissed the espionage concerns. The company also said the United States should be promoting competition.\\nThe company has won four major U.S. rail car contracts. It is pursuing a Metro contract worth more than $1 billion to build up to 800 of the new rail cars.\\nThere are no U.S. transit rail car manufacturers. The bidding deadline is May 31.\\n\", \"domain\": \"theepochtimes.com\", \"title\": \"US Senators Threaten Metro Funding Over Chinese Manufacturer\", \"date\": \"April 15, 2019\", \"authors\": null, \"ind30k\": 29363, \"url\": \"https://www.theepochtimes.com/us-senators-threaten-metro-funding-over-chinese-manufacturer_2880764.html\", \"label\": \"human\", \"orig_split\": \"train_burner\", \"split\": \"train\", \"random_score\": -4.26355665680627}\n",
            "\n",
            "l: {\"article\": \"Transparent may have been renewed for a third season on Amazon just last week, but Jeffrey Tambor\\u2018s character is already getting the ax. In a musical send-off for its beloved trans character Maura Pfefferman, the show is planning a short episode where fans will bid farewell to Maura and the Pfefferman family by serenading her with a song.\\nAlong with news that Transparent will be renewed for a third season in the near future, Amazon announced that it will hold an aftershow in which fans will share memories and songs in honor of the show\\u2019s aging lead character Maura Pfefferman.\\nTransparent: We Know Goodbye\\nFollowing the episode \\u201cWe Know Goodbye,\\u201d viewers will get to watch fans reminisce about the beloved character, who was introduced in the third season as her father Mort in the show\\u2019s first episode. The short episode will culminate with a rendition of \\u201cI Won\\u2019t Hate You Anymore,\\u201d a hit in 1984\\u2019s Coming To America. And yes, it was originally the song performed by Eddie Murphy in the Eddie Murphy film that inspired the Transparent song:\\nAmazon\\u2019s synopsis for the episode read:\\nIn what will be the final episode of the show, Maura (Jeffrey Tambor) bids farewell to her family as she fades away into memory. With memories of their first encounter (and their shared encounter with God) the cast and crew create a musical play, complete with tears, solos, duets, a song about Mufasa (aka she has sex with a gorilla), and a comedic karaoke extravaganza.\\nFans have shared their favorite moments from Transparent throughout the past two seasons and will now be able to make those memories in person.\\nThis isn\\u2019t the first time that Amazon has launched an aftershow, following in the footsteps of Transparent\\u2018s much buzzed-about aftershow \\u201c20% Somethings.\\u201d It\\u2019s a neat move for a show that has been so beloved, and for fans that are so attached to the Pfefferman family.\\nDana Brunetti, Netflix\\u2019s executive producer and the company\\u2019s movie division heads, once told the Hollywood Reporter that Netflix now limits the amount of hours Netflix subscribers can watch an entire season of their favorite shows:\\nIt used to be that you had to watch the whole season, but now it\\u2019s maxed out at a certain time. If you want to watch the whole season of House of Cards in one night, you have to wait at least a week. Now that we\\u2019re more selective with what we give out on this balance, people are getting more time with their favorite shows.\\nI guess we can thank Netflix for forcing us to wait weeks to binge on all the great TV shows we love. Though I wouldn\\u2019t mind picking up a shopping cart for the Netflix app as we wait.\\n\", \"domain\": \"slashfilm.com\", \"title\": \"\\u2018Transparent\\u2019 to Kill Off Jeffrey Tambor\\u2019s Character in Musical Finale\", \"date\": \"April 11, 2019\", \"authors\": \"Hoai-Tran Bui\", \"ind30k\": 4999, \"url\": \"https://www.slashfilm.com/transparent-jeffrey-tambor-dead-finale/\", \"label\": \"machine\", \"orig_split\": \"gen\", \"top_p\": 0.9396825396825397, \"split\": \"train\"}\n",
            "\n",
            "l: {\"article\": \"In December 2018 the issue of why the Canadian government declined to purchase a second interim supply ship \\u2013 a sister ship to MV Asterix which is currently at sea supporting Royal Canadian Navy operations \\u2013 became a source of debate in the Commons.\\nConservative leader Andrew Scheer questioned the Liberal government why it was not moving ahead with having Davie provide the second supply ship, the Obelix, to the navy. Scheer said the navy needed the second ship.\\nBut Prime Minister Justin Trudeau accused Scheer of playing \\u201cpetty politics.\\u201d\\n\\u201cThe armed forces did an assessment,\\u201d Trudeau explained. \\u201cThey don\\u2019t need the Obelix and for him to suggest that we should buy it anyway is pure base politics, the worst politics. We make our decisions based on facts. We recognize the quality of work done by Davie shipyard and we do want them to get good jobs but we are not going to make up work for political reasons.\\u201d\\nIn February, the Conservatives again went after the issue of the second supply ship and questioned whether the assessment referred to by Trudeau even existed.\\nThat is a good question.\\nWhen asked about the issue the RCN noted in an email Dec. 20, 2018 to Defence Watch that in November 2014, the navy outlined the high level requirements for an interim AOR capability in a document entitled: Interim AOR (iAOR) \\u2013 High Level Requirements (HLRs). \\u201cThe specific requirement was detailed in terms of the number of sea days per year and a certain set of key capabilities to be provided, the RCN noted in its email. \\u201cThis requirement was intended to meet the RCN needs while progress was made on the Joint Support Ship project which will deliver two JSS PROTECTEUR Class ships as directed in the defence policy Strong, Secure, Engaged. There has been no change to the RCN requirements.\\u201d\\nDefence Watch reviewed that 2014 document and it does not discuss a second interim supply ship, let alone rule out the need for a second vessel. It continually refers to the criteria for a single supply ship.\\nThere are also a number of other factors to consider here. On May 30 2018, Vice Admiral Darren Hawco testified before the Senate\\u2019s National Finance Committee. He stated: \\u201cWe never really looked at the need for or validated the need for a second interim AOR.\\u201d\\nDavie officials argue the Liberal\\u2019s own Strong Secure and Engaged defence policy points to the need for that second ship. The policy calls for two supply ships, one on each coast, operating simultaneously. Currently there is only one ship, MV Asterix. The first new Joint Support Ship isn\\u2019t expected for another four years.\\nDavie also noted that the money for a second ship is available, pointing out that the Department of National Defence lapsed between $2 billion to $3 billion in 2017/2018. The second supply ship is being offered by Davie for $500 million.\\nScheer has committed to building a second interim supply ship if his party forms the next government.\\nSome further background reading:\\nhttps://ottawacitizen.com/news/national/defence-watch/the-case-for-a-second-interim-resupply-ship-new-stats-show-how-foreign-navies-continue-to-refuel-canadian-navy-ships\\n(Analysis)\\n\", \"domain\": \"ottawacitizen.com\", \"title\": \"Military assessment ruling out the need for a second interim supply ship doesn\\u2019t appear to exist\", \"date\": \"April 16, 2019\", \"authors\": \"Updated\", \"ind30k\": 23090, \"url\": \"https://ottawacitizen.com/news/national/defence-watch/military-assessment-supposedly-ruling-out-a-second-interim-supply-ship-doesnt-appear-to-exist\", \"label\": \"human\", \"orig_split\": \"train_burner\", \"split\": \"train\", \"random_score\": -3.8636238954221143}\n",
            "\n",
            "l: {\"article\": \"Modi also said the opposition wants to strip the armed forces of their special powers, while the NDA government strives to give full liberty to jawans to deal with terrorists and Naxals. (Image: ANI twitter)\\nBhagalpur: Prime Minister Narendra Modi on Thursday launched a scathing attack on the Congress-led opposition, asserting that the \\\"mahamilavati gang\\\" is scared that if he comes to power again, their \\\"shops\\\" of corruption and dynasty politics will shut down.\\nAddressing an election rally here, Modi also said the opposition wants to strip the armed forces of their special powers, while the NDA government strives to give full liberty to jawans to deal with terrorists and Naxals. \\\"The mahamilavati gang is actually afraid of something else though it may have other pretensions while scaremongering. They fear that if Modi comes to power again, their shops of corruption, dynasty politics and venal defence deals will shut down,\\\" he said.\\nThe Prime Minister said his government has been making efforts to bolster the quota system introduced by Baba Saheb Ambedkar. \\\"They are saying that if Modi comes to power again, elections would be done away with. All constitutional bodies will be under threat. And reservations will be done away with. \\\"The fact remains, this 'chowkidar' of yours has been making all efforts to strengthen the quota system introduced by Baba Saheb Ambedkar,\\\" Modi said.\\nBihar will witness a seven-phase polling for its 40 Lok Sabha seats.\\nKeep yourself updated on Lok Sabha Elections 2019 with our round-the-clock coverage -- breaking news, updates, analyses et all. Happy reading.\\n\", \"domain\": \"deccanchronicle.com\", \"title\": \"Mahamilavat gang scared I will shut their shops of corruption if back in power: Modi\", \"date\": \"April 11, 2019\", \"authors\": null, \"ind30k\": 28881, \"url\": \"https://www.deccanchronicle.com/nation/politics/110419/opposition-scared-indulging-in-scaremongering-modi.html\", \"label\": \"human\", \"orig_split\": \"train_burner\", \"split\": \"train\", \"random_score\": -3.8137245194108234}\n",
            "\n",
            "examples[train]: 4\n",
            "examples[val]: 0\n",
            "examples[test]: 0\n",
            "INFO:tensorflow:*** Done parsing files ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44LkfjWHQQpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "if FLAGS.max_training_examples > 0:\n",
        "  print(\"*** Setting max_training_examples ***\", flush=True)\n",
        "  examples_by_label = {'human': [], 'machine': []}\n",
        "  for x in examples['train']:\n",
        "      examples_by_label[x['label']].append(x)\n",
        "\n",
        "  new_examples = []\n",
        "  print(\"Unique machine examples: {} -> {}\".format(len(examples_by_label['machine']),\n",
        "                                                    FLAGS.max_training_examples), flush=True)\n",
        "  machine_ex_to_keep = examples_by_label['machine'][:FLAGS.max_training_examples]\n",
        "\n",
        "  # So we just cut down on the TRUE machine examples. now lets try adding in additional examples\n",
        "  # examples_by_label['human'].extend(additional_data['human'])\n",
        "\n",
        "  if len(additional_data['machine']) > 0:\n",
        "      amount_to_add = len(examples_by_label['human']) - len(machine_ex_to_keep)\n",
        "      if amount_to_add > 0:\n",
        "          machine_ex_to_keep.extend(additional_data['machine'][:amount_to_add])\n",
        "\n",
        "  for i, human_ex in enumerate(examples_by_label['human']):\n",
        "      new_examples.append(human_ex)\n",
        "      new_examples.append(machine_ex_to_keep[i % len(machine_ex_to_keep)])\n",
        "\n",
        "  print(\"Length of examples: {} -> {}\".format(len(examples['train']), len(new_examples)), flush=True)\n",
        "  examples['train'] = new_examples\n",
        "  print(\"*** Done Setting max_training_examples\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrsOQUa1Re6b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b0826f18-14ee-4fbe-95dd-5d258ec9b3ff"
      },
      "source": [
        "\n",
        "# Training\n",
        "if FLAGS.do_train:\n",
        "    print(\"examples[train]: \" + str(len(examples['train'])))\n",
        "    print(\"examples[val]: \" + str(len(examples['val'])))\n",
        "    print(\"examples[test]: \" + str(len(examples['test'])))\n",
        "    #num_train_steps = int((len(examples['train']) / FLAGS.batch_size) * FLAGS.num_train_epochs)\n",
        "    num_train_steps = int((len(examples['train']) / 2) * FLAGS.num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
        "    assert num_train_steps > 0\n",
        "else:\n",
        "    num_train_steps = None\n",
        "    num_warmup_steps = None\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "examples[train]: 4\n",
            "examples[val]: 0\n",
            "examples[test]: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDOwHmw1Q9hv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "9f7e9611-ec53-462c-e20b-5445499a11e5"
      },
      "source": [
        "\n",
        "# Boilerplate\n",
        "tpu_cluster_resolver = None\n",
        "if FLAGS.use_tpu and FLAGS.tpu_name:\n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
        "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
        "\n",
        "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    master=FLAGS.master,\n",
        "    model_dir=FLAGS.output_dir,\n",
        "    save_checkpoints_steps=FLAGS.iterations_per_loop,\n",
        "    keep_checkpoint_max=None,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=FLAGS.iterations_per_loop,\n",
        "        num_shards=FLAGS.num_tpu_cores,\n",
        "        per_host_input_for_training=is_per_host))\n",
        "\n",
        "model_fn = classification_model_fn_builder(\n",
        "    news_config,\n",
        "    init_checkpoint=FLAGS.init_checkpoint,\n",
        "    learning_rate=FLAGS.learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=FLAGS.use_tpu,\n",
        "    num_labels=len(LABEL_LIST),\n",
        "    pool_token_id=encoder.begin_summary,\n",
        "    adafactor=FLAGS.adafactor\n",
        ")\n",
        "\n",
        "# If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "# or GPU.\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=FLAGS.use_tpu,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=FLAGS.batch_size,\n",
        "    eval_batch_size=FLAGS.batch_size,\n",
        "    predict_batch_size=FLAGS.batch_size,\n",
        "    params={'model_dir': FLAGS.output_dir}\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/content/gdrive/My Drive/NLP_Projects/grover-master/discrimination/output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': None, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1cdbb40c50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqy9YUQ0I0Xu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a15d0dec-e673-45a3-95d3-dc0cd2168665"
      },
      "source": [
        "if FLAGS.do_train:\n",
        "    train_file = os.path.join(FLAGS.output_dir, \"train.tf_record\")\n",
        "\n",
        "    tf.logging.info(f\"***** Recreating training file at {train_file} *****\")\n",
        "#    classification_convert_examples_to_features(examples['train'], batch_size=FLAGS.batch_size,\n",
        "#                                                max_seq_length=FLAGS.max_seq_length,\n",
        "#                                                encoder=encoder, output_file=train_file,\n",
        "#                                                labels=LABEL_LIST,\n",
        "#                                                chop_from_front_if_needed=False)\n",
        "    classification_convert_examples_to_features(examples['train'], batch_size=FLAGS.batch_size,\n",
        "                                            max_seq_length=FLAGS.max_seq_length,\n",
        "                                            encoder=encoder, output_file=train_file,\n",
        "                                            labels=LABEL_LIST,\n",
        "                                            chop_from_front_if_needed=False)\n",
        "\n",
        "    tf.logging.info(\"***** Running training *****\")\n",
        "    tf.logging.info(\"  Num examples = %d\", len(examples['train']))\n",
        "    tf.logging.info(\"  Num epochs = %d\", FLAGS.num_train_epochs)\n",
        "    tf.logging.info(\"  Batch size = %d\", FLAGS.batch_size)\n",
        "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "\n",
        "    train_input_fn = classification_input_fn_builder(input_file=train_file, seq_length=FLAGS.max_seq_length,\n",
        "                                                      is_training=True, drop_remainder=True,\n",
        "                                                      )\n",
        "    estimator.train(input_fn=train_input_fn, steps=num_train_steps)\n",
        "\n",
        "splits_to_predict = [x for x in ['val', 'test'] if getattr(FLAGS, f'predict_{x}')]\n",
        "for split in splits_to_predict:\n",
        "    num_actual_examples = len(examples[split])\n",
        "\n",
        "    predict_file = os.path.join(FLAGS.output_dir, f'{split}.tf_record')\n",
        "    tf.logging.info(f\"***** Recreating {split} file {predict_file} *****\")\n",
        "    classification_convert_examples_to_features(examples[split], batch_size=FLAGS.batch_size,\n",
        "                                                max_seq_length=FLAGS.max_seq_length,\n",
        "                                                encoder=encoder, output_file=predict_file,\n",
        "                                                labels=LABEL_LIST, pad_extra_examples=True,\n",
        "                                                chop_from_front_if_needed=False)\n",
        "\n",
        "    val_input_fn = classification_input_fn_builder(input_file=predict_file, seq_length=FLAGS.max_seq_length,\n",
        "                                                    is_training=False, drop_remainder=True,\n",
        "                                                    )\n",
        "\n",
        "    probs = np.zeros((num_actual_examples, 2), dtype=np.float32)\n",
        "    for i, res in enumerate(estimator.predict(input_fn=val_input_fn, yield_single_examples=True)):\n",
        "        if i < num_actual_examples:\n",
        "            probs[i] = res['probs']\n",
        "\n",
        "    _save_np(os.path.join(FLAGS.output_dir, f'{split}-probs.npy'), probs)\n",
        "\n",
        "    preds = np.argmax(probs, 1)\n",
        "    labels = np.array([LABEL_INV_MAP[x['label']] for x in examples[split][:num_actual_examples]])\n",
        "    print('{} ACCURACY IS {:.3f}'.format(split, np.mean(labels == preds)), flush=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:***** Recreating training file at /content/gdrive/My Drive/NLP_Projects/grover-master/discrimination/output/train.tf_record *****\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/dataloader.py:94: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "INFO:tensorflow:Writing example 0 of 4\n",
            "INFO:tensorflow:***** Running training *****\n",
            "INFO:tensorflow:  Num examples = 4\n",
            "INFO:tensorflow:  Num epochs = 3\n",
            "INFO:tensorflow:  Batch size = 32\n",
            "INFO:tensorflow:  Num steps = 6\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/dataloader.py:137: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/dataloader.py:157: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Running train on CPU\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (32, 1024)\n",
            "INFO:tensorflow:  name = is_real_example, shape = (32,)\n",
            "INFO:tensorflow:  name = label_ids, shape = (32,)\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/modeling.py:490: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/modeling.py:296: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/modeling.py:302: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/utils.py:125: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/modeling.py:143: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/utils.py:112: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/optimization_adafactor.py:22: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/optimization_adafactor.py:27: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/optimization_adafactor.py:65: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "INFO:tensorflow:  name = newslm/embeddings/word_embed:0, shape = (50270, 768)\n",
            "INFO:tensorflow:  name = newslm/embeddings/pos_embed:0, shape = (1024, 768)\n",
            "INFO:tensorflow:  name = newslm/embeddings/LayerNorm_embed_norm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/embeddings/LayerNorm_embed_norm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer00/query_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer00/query_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer00/key_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer00/key_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer00/value_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer00/value_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer00/context_projection_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer00/context_projection_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer00/LayerNorm_mlp_ln0/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer00/LayerNorm_mlp_ln0/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer00/intermediate/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = newslm/layer00/intermediate/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = newslm/layer00/output/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = newslm/layer00/output/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer00/LayerNorm_mlp_ln1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer00/LayerNorm_mlp_ln1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer01/query_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer01/query_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer01/key_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer01/key_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer01/value_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer01/value_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer01/context_projection_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer01/context_projection_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer01/LayerNorm_mlp_ln0/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer01/LayerNorm_mlp_ln0/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer01/intermediate/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = newslm/layer01/intermediate/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = newslm/layer01/output/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = newslm/layer01/output/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer01/LayerNorm_mlp_ln1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer01/LayerNorm_mlp_ln1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer02/query_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer02/query_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer02/key_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer02/key_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer02/value_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer02/value_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer02/context_projection_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer02/context_projection_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer02/LayerNorm_mlp_ln0/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer02/LayerNorm_mlp_ln0/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer02/intermediate/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = newslm/layer02/intermediate/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = newslm/layer02/output/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = newslm/layer02/output/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer02/LayerNorm_mlp_ln1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer02/LayerNorm_mlp_ln1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer03/query_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer03/query_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer03/key_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer03/key_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer03/value_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer03/value_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer03/context_projection_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer03/context_projection_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer03/LayerNorm_mlp_ln0/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer03/LayerNorm_mlp_ln0/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer03/intermediate/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = newslm/layer03/intermediate/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = newslm/layer03/output/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = newslm/layer03/output/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer03/LayerNorm_mlp_ln1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer03/LayerNorm_mlp_ln1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer04/query_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer04/query_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer04/key_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer04/key_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer04/value_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer04/value_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer04/context_projection_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer04/context_projection_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer04/LayerNorm_mlp_ln0/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer04/LayerNorm_mlp_ln0/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer04/intermediate/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = newslm/layer04/intermediate/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = newslm/layer04/output/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = newslm/layer04/output/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer04/LayerNorm_mlp_ln1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer04/LayerNorm_mlp_ln1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer05/query_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer05/query_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer05/key_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer05/key_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer05/value_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer05/value_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer05/context_projection_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer05/context_projection_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer05/LayerNorm_mlp_ln0/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer05/LayerNorm_mlp_ln0/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer05/intermediate/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = newslm/layer05/intermediate/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = newslm/layer05/output/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = newslm/layer05/output/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer05/LayerNorm_mlp_ln1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer05/LayerNorm_mlp_ln1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer06/query_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer06/query_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer06/key_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer06/key_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer06/value_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer06/value_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer06/context_projection_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer06/context_projection_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer06/LayerNorm_mlp_ln0/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer06/LayerNorm_mlp_ln0/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer06/intermediate/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = newslm/layer06/intermediate/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = newslm/layer06/output/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = newslm/layer06/output/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer06/LayerNorm_mlp_ln1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer06/LayerNorm_mlp_ln1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer07/query_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer07/query_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer07/key_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer07/key_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer07/value_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer07/value_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer07/context_projection_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer07/context_projection_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer07/LayerNorm_mlp_ln0/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer07/LayerNorm_mlp_ln0/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer07/intermediate/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = newslm/layer07/intermediate/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = newslm/layer07/output/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = newslm/layer07/output/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer07/LayerNorm_mlp_ln1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer07/LayerNorm_mlp_ln1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer08/query_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer08/query_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer08/key_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer08/key_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer08/value_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer08/value_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer08/context_projection_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer08/context_projection_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer08/LayerNorm_mlp_ln0/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer08/LayerNorm_mlp_ln0/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer08/intermediate/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = newslm/layer08/intermediate/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = newslm/layer08/output/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = newslm/layer08/output/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer08/LayerNorm_mlp_ln1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer08/LayerNorm_mlp_ln1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer09/query_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer09/query_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer09/key_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer09/key_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer09/value_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer09/value_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer09/context_projection_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer09/context_projection_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer09/LayerNorm_mlp_ln0/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer09/LayerNorm_mlp_ln0/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer09/intermediate/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = newslm/layer09/intermediate/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = newslm/layer09/output/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = newslm/layer09/output/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer09/LayerNorm_mlp_ln1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer09/LayerNorm_mlp_ln1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer10/query_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer10/query_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer10/key_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer10/key_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer10/value_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer10/value_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer10/context_projection_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer10/context_projection_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer10/LayerNorm_mlp_ln0/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer10/LayerNorm_mlp_ln0/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer10/intermediate/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = newslm/layer10/intermediate/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = newslm/layer10/output/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = newslm/layer10/output/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer10/LayerNorm_mlp_ln1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer10/LayerNorm_mlp_ln1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer11/query_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer11/query_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer11/key_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer11/key_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer11/value_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer11/value_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer11/context_projection_layer/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = newslm/layer11/context_projection_layer/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer11/LayerNorm_mlp_ln0/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer11/LayerNorm_mlp_ln0/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer11/intermediate/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = newslm/layer11/intermediate/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = newslm/layer11/output/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = newslm/layer11/output/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer11/LayerNorm_mlp_ln1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = newslm/layer11/LayerNorm_mlp_ln1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = classification/logits/kernel:0, shape = (768, 2)\n",
            "INFO:tensorflow:  name = classification/logits/bias:0, shape = (2,)\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/modeling.py:918: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/NLP_Projects/grover-master/lm/modeling.py:918: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
            "\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/NLP_Projects/grover-master/discrimination/output/model.ckpt-0\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /content/gdrive/My Drive/NLP_Projects/grover-master/discrimination/output/model.ckpt.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0LGKG7dJzeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}