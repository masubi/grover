{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grover-Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masubi/grover/blob/master/Grover_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1ANYnHVvfeW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6d936eed-02b8-43ea-c79d-af31e690e530"
      },
      "source": [
        "#!pip install bert-pytorch\n",
        "!pip install absl-py"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skx6hMPixnF5",
        "colab_type": "code",
        "outputId": "7a4217bf-9ea5-4d19-df0b-67dfbd60e142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id43yVeR0l4m",
        "colab_type": "code",
        "outputId": "574a2ab0-a656-45a9-f7d1-878b6fa17303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/NLP_Projects/grover-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcyvLmkL0vLq",
        "colab_type": "code",
        "outputId": "9b6e5342-bbc8-4783-84ce-8499d4be3f0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/NLP_Projects/grover-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8Y90vzZ6zeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python3 download_model.py base #run if it hasn't downloaded model yet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87ZYWw0o1P2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "For discrimination finetuning (e.g. saying whether or not the generation is human/grover)\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.lib.io import file_io\n",
        "\n",
        "from lm.dataloader import classification_convert_examples_to_features, classification_input_fn_builder\n",
        "from lm.modeling import classification_model_fn_builder, GroverConfig\n",
        "from lm.utils import _save_np\n",
        "from sample.encoder import get_encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5gIvfE61dS4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "903cb87b-a0e3-44b8-8b4d-fa63db6884d6"
      },
      "source": [
        "flags = tf.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "#\n",
        "# Clearing flags in this section so can rerun,  FLAGS are global which is why\n",
        "# error kept occurring \n",
        "#\n",
        "configListToRemove = [\"config_file\", \n",
        "                      \"input_data\", \n",
        "                      \"additional_data\", \n",
        "                      \"output_dir\",\n",
        "                      \"init_checkpoint\",\n",
        "                      \"max_seq_length\",\n",
        "                      \"iterations_per_loop\",\n",
        "                      \"batch_size\",\n",
        "                      \"max_training_examples\",\n",
        "                      \"do_train\",\n",
        "                      \"predict_val\",\n",
        "                      \"predict_test\",\n",
        "                      \"num_train_epochs\",\n",
        "                      \"warmup_proportion\",\n",
        "                      \"adafactor\",\n",
        "                      \"learning_rate\",\n",
        "                      \"use_tpu\",\n",
        "                      \"tpu_name\",\n",
        "                      \"tpu_zone\",\n",
        "                      \"gcp_project\",\n",
        "                      \"master\",\n",
        "                      \"num_tpu_cores\",\n",
        "                      \"f\"\n",
        "                      ]\n",
        "                \n",
        "for name in list(flags.FLAGS):\n",
        "  if(name in configListToRemove):\n",
        "    delattr(flags.FLAGS,name)\n",
        "    print(\"pre-delAttr: \" + name)\n",
        "\n",
        "#for name in list(flags.FLAGS):\n",
        "#  \n",
        "## Required parameters\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"config_file\", 'lm/configs/base.json',\n",
        "    \"The config json file corresponding to the pre-trained news model. \"\n",
        "    \"This specifies the model architecture.\")\n",
        "      \n",
        "flags.DEFINE_string(\n",
        "    \"input_data\", '/content/gdrive/My Drive/NLP_Projects/grover-master/discrimination/input/',\n",
        "    \"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"additional_data\", None,\n",
        "    \"Should we provide additional input data? maybe.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"output_dir\", '/content/gdrive/My Drive/NLP_Projects/grover-master/discrimination/output/',\n",
        "    \"The output directory where the model checkpoints will be written.\")\n",
        "\n",
        "## Other parameters\n",
        "flags.DEFINE_string(\n",
        "    \"init_checkpoint\", None,\n",
        "    \"Initial checkpoint (usually from a pre-trained model).\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_seq_length\", 1024,\n",
        "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
        "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
        "    \"than this will be padded. Must match data generation.\")\n",
        "\n",
        "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
        "                     \"How many steps to make in each estimator call.\")\n",
        "\n",
        "flags.DEFINE_integer(\"batch_size\", 32, \"Batch size used\")\n",
        "\n",
        "flags.DEFINE_integer(\"max_training_examples\", -1, \"if you wanna limit the number\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n",
        "\n",
        "flags.DEFINE_bool(\"predict_val\", False, \"Whether to run eval on the dev set.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"predict_test\", False,\n",
        "    \"Whether to run the model in inference mode on the test set.\")\n",
        "\n",
        "flags.DEFINE_float(\"num_train_epochs\", 3.0,\n",
        "                   \"Total number of training epochs to perform.\")\n",
        "\n",
        "flags.DEFINE_float(\n",
        "    \"warmup_proportion\", 0.1,\n",
        "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
        "    \"E.g., 0.1 = 10% of training.\")\n",
        "\n",
        "flags.DEFINE_bool(\"adafactor\", False, \"Whether to run adafactor\")\n",
        "\n",
        "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
        "\n",
        "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"tpu_name\", None,\n",
        "    \"The Cloud TPU to use for training. This should be either the name \"\n",
        "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
        "    \"url.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"tpu_zone\", None,\n",
        "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"gcp_project\", None,\n",
        "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"num_tpu_cores\", 8,\n",
        "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "print(\"flags loaded\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pre-delAttr: config_file\n",
            "pre-delAttr: input_data\n",
            "pre-delAttr: additional_data\n",
            "pre-delAttr: output_dir\n",
            "pre-delAttr: init_checkpoint\n",
            "pre-delAttr: max_seq_length\n",
            "pre-delAttr: iterations_per_loop\n",
            "pre-delAttr: batch_size\n",
            "pre-delAttr: max_training_examples\n",
            "pre-delAttr: do_train\n",
            "pre-delAttr: predict_val\n",
            "pre-delAttr: predict_test\n",
            "pre-delAttr: num_train_epochs\n",
            "pre-delAttr: warmup_proportion\n",
            "pre-delAttr: adafactor\n",
            "pre-delAttr: learning_rate\n",
            "pre-delAttr: use_tpu\n",
            "pre-delAttr: tpu_name\n",
            "pre-delAttr: tpu_zone\n",
            "pre-delAttr: gcp_project\n",
            "pre-delAttr: master\n",
            "pre-delAttr: num_tpu_cores\n",
            "pre-delAttr: f\n",
            "flags loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hepD3i8u-jx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _flatten_and_tokenize_metadata(encoder, item):\n",
        "    \"\"\"\n",
        "    Turn the article into tokens\n",
        "    :param item: Contains things that need to be tokenized\n",
        "\n",
        "    fields are ['domain', 'date', 'authors', 'title', 'article', 'summary']\n",
        "    :return: dict\n",
        "    \"\"\"\n",
        "    metadata = []\n",
        "    for key in ['domain', 'date', 'authors', 'title', 'article']:\n",
        "        val = item.get(key, None)\n",
        "        if val is not None:\n",
        "            metadata.append(encoder.__dict__[f'begin_{key}'])\n",
        "            metadata.extend(encoder.encode(val))\n",
        "            metadata.append(encoder.__dict__[f'end_{key}'])\n",
        "    return metadata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50NqMhI1-xBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "LABEL_LIST = ['machine', 'human']\n",
        "LABEL_INV_MAP = {label: i for i, label in enumerate(LABEL_LIST)}\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2u9kQooH280",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07f83d02-8374-473b-e1b5-69c70e7d5c1f"
      },
      "source": [
        "\n",
        "def checkFileConfigs():\n",
        "  if tf.gfile.Exists(FLAGS.output_dir):\n",
        "      print(f\"The output directory {FLAGS.output_dir} exists!\")\n",
        "      #if FLAGS.do_train:\n",
        "      #    print(\"EXITING BECAUSE DO_TRAIN is true\", flush=True)\n",
        "      #       return\n",
        "      for split in ['val', 'test']:\n",
        "          if tf.gfile.Exists(os.path.join(FLAGS.output_dir, f'{split}-probs.npy')) and getattr(FLAGS,\n",
        "                                                                                                f'predict_{split}'):\n",
        "              print(f\"EXITING BECAUSE {split}-probs.npy exists\", flush=True)\n",
        "              return\n",
        "  elif not FLAGS.do_train:\n",
        "      print(\"EXITING BECAUSE DO_TRAIN IS FALSE AND PATH DOESNT EXIST\")\n",
        "      return\n",
        "  #else:\n",
        "      #tf.gfile.MakeDirs(FLAGS.output_dir)\n",
        "  print(\"complete checking\")\n",
        "\n",
        "checkFileConfigs()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "complete checking\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCo4LyO_-1iU",
        "colab_type": "code",
        "outputId": "2f530e0f-0d0c-41a6-82cc-8ed765addbfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "news_config = GroverConfig.from_json_file(FLAGS.config_file)\n",
        "\n",
        "# TODO might have to change this\n",
        "encoder = get_encoder()\n",
        "examples = {'train': [], 'val': [], 'test': []}\n",
        "np.random.seed(123456)\n",
        "tf.logging.info(\"*** Parsing files ***\")\n",
        "with tf.gfile.Open(FLAGS.input_data, \"r\") as f:\n",
        "    for l in f:\n",
        "        item = json.loads(l)\n",
        "\n",
        "        # This little hack is because we don't want to tokenize the article twice\n",
        "        context_ids = _flatten_and_tokenize_metadata(encoder=encoder, item=item)\n",
        "        examples[item['split']].append({\n",
        "            'info': item,\n",
        "            'ids': context_ids,\n",
        "            'label': item['label'],\n",
        "        })\n",
        "        assert item['label'] in LABEL_INV_MAP\n",
        "\n",
        "additional_data = {'machine': [], 'human': []}\n",
        "if FLAGS.additional_data is not None:\n",
        "    print(\"NOW WERE LOOKING AT ADDITIONAL INPUT DATA\", flush=True)\n",
        "    with tf.gfile.Open(FLAGS.additional_data, \"r\") as f:\n",
        "        for l in f:\n",
        "            item = json.loads(l)\n",
        "            # This little hack is because we don't want to tokenize the article twice\n",
        "            context_ids = _flatten_and_tokenize_metadata(encoder=encoder, item=item)\n",
        "            additional_data[item['label']].append({\n",
        "                'info': item,\n",
        "                'ids': context_ids,\n",
        "                'label': item['label'],\n",
        "            })\n",
        "\n",
        "tf.logging.info(\"*** Done parsing files ***\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Parsing files ***\n",
            "INFO:tensorflow:*** Done parsing files ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44LkfjWHQQpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "if FLAGS.max_training_examples > 0:\n",
        "  print(\"*** Setting max_training_examples ***\", flush=True)\n",
        "  examples_by_label = {'human': [], 'machine': []}\n",
        "  for x in examples['train']:\n",
        "      examples_by_label[x['label']].append(x)\n",
        "\n",
        "  new_examples = []\n",
        "  print(\"Unique machine examples: {} -> {}\".format(len(examples_by_label['machine']),\n",
        "                                                    FLAGS.max_training_examples), flush=True)\n",
        "  machine_ex_to_keep = examples_by_label['machine'][:FLAGS.max_training_examples]\n",
        "\n",
        "  # So we just cut down on the TRUE machine examples. now lets try adding in additional examples\n",
        "  # examples_by_label['human'].extend(additional_data['human'])\n",
        "\n",
        "  if len(additional_data['machine']) > 0:\n",
        "      amount_to_add = len(examples_by_label['human']) - len(machine_ex_to_keep)\n",
        "      if amount_to_add > 0:\n",
        "          machine_ex_to_keep.extend(additional_data['machine'][:amount_to_add])\n",
        "\n",
        "  for i, human_ex in enumerate(examples_by_label['human']):\n",
        "      new_examples.append(human_ex)\n",
        "      new_examples.append(machine_ex_to_keep[i % len(machine_ex_to_keep)])\n",
        "\n",
        "  print(\"Length of examples: {} -> {}\".format(len(examples['train']), len(new_examples)), flush=True)\n",
        "  examples['train'] = new_examples\n",
        "  print(\"*** Done Setting max_training_examples\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrsOQUa1Re6b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "abfca3ef-224e-4ecb-f0bd-6c97a94c3db2"
      },
      "source": [
        "\n",
        "# Training\n",
        "if FLAGS.do_train:\n",
        "    print(\"examples: \" + str(len(examples['train'])))\n",
        "    #num_train_steps = int((len(examples['train']) / FLAGS.batch_size) * FLAGS.num_train_epochs)\n",
        "    num_train_steps = int((len(examples['train']) / 2) * FLAGS.num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
        "    assert num_train_steps > 0\n",
        "else:\n",
        "    num_train_steps = None\n",
        "    num_warmup_steps = None\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "examples: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDOwHmw1Q9hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Boilerplate\n",
        "tpu_cluster_resolver = None\n",
        "if FLAGS.use_tpu and FLAGS.tpu_name:\n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
        "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
        "\n",
        "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    master=FLAGS.master,\n",
        "    model_dir=FLAGS.output_dir,\n",
        "    #model_dir=\"./discrimination/output/checkpoint/\",\n",
        "    save_checkpoints_steps=FLAGS.iterations_per_loop,\n",
        "    keep_checkpoint_max=None,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=FLAGS.iterations_per_loop,\n",
        "        num_shards=FLAGS.num_tpu_cores,\n",
        "        per_host_input_for_training=is_per_host))\n",
        "\n",
        "model_fn = classification_model_fn_builder(\n",
        "    news_config,\n",
        "    init_checkpoint=FLAGS.init_checkpoint,\n",
        "    learning_rate=FLAGS.learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=FLAGS.use_tpu,\n",
        "    num_labels=len(LABEL_LIST),\n",
        "    pool_token_id=encoder.begin_summary,\n",
        "    adafactor=FLAGS.adafactor\n",
        ")\n",
        "\n",
        "# If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "# or GPU.\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=FLAGS.use_tpu,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=FLAGS.batch_size,\n",
        "    eval_batch_size=FLAGS.batch_size,\n",
        "    predict_batch_size=FLAGS.batch_size,\n",
        "    params={'model_dir': FLAGS.output_dir}\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}